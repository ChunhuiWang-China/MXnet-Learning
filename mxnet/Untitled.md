1:算法的主要目的是成本函数中w，和b最小，损失函数是针对特征个体的，而成本函数则是正对全局的。

2: 剃度下降，其实就是对成本函数求导，求导是为了活的在w，b着个点的斜率，斜率是下降速度对快的方向，这样在乘上步长，每次迭代就可以进行慢慢的收敛

3: 神经网络中，过拟合导致高方差。欠拟合导致偏差过高

偏差降低的方法，选择不同的网络

方差降低的方法，选择更多的数据，或者正则化，dropout（随机失效）反向随机失活：使用dropout要保证均值不变，所以要除以dropout的keep－prob概率。

CNN： pending 添加像素点来防止丢失边缘像素。p ＝ （f－1）／2 
		卷积进行边缘监测，获取明亮区域
		步长：卷积一次移动的长度
		池化： 平均池化，最大池化，很少用padding，计算和形式根卷机差不多，但是是一个静态属性
		
		
		LeNet－5 1998年提出，使用平均池化，没有使用pending，并且使用5x5x6的卷机和2x2的池化。
		AlexNet 根LeNet类似，不过使用了pending和最大池化，同时参数变大了。使用了ReLu激活函数
		VGG－16 结构简单，但是超参数1.3e
		RestNet  残差网络
		